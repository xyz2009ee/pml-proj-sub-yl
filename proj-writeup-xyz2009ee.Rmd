---
title: "pml-xyz2009ee"
output: html_document
---


First of all, the training and testing data sets has to be preprocessed. The variables that has more than 80% of 'NA' will not be used for training or testing data sets.

Then each of the user in this data set is trained seperately and the prediction on the classe is based on the model trained for each of the users. The reason to build seperate model for each user is the homogeneity for the same user.

The generalized linear model is used for training and the glmnet package has to be loaded. The cross validation in the glmnet package is 10 fold cross validation. 
The fitting accuracy for this model is 98.1% for carlitos, 95.8% for petro, 93.8% for adelmo, 95.8% for charles, 95.0% for eurico, 94.7% for jeremy. Therefore, the in sample error is 1.9% for carlitos, 4,2% for for petro, 6.2% for adelmo, 4.2% for charles, 5.0% for eurico and 5.3% for jeremy. The out of sample is always greater than in sample error, therefore we have the lower bound for the out of sample error. In addition, by applying the inequality $P(|E_{in}-E_{out}|<\epsilon)<= 4(2N)^{d_{vc}}e^{-\frac{1}{8}\epsilon^{2}N}$, we can also estimate an upper bound for the out of sample error with certain probablity $\epsilon$. Here $d_{vc}$ denotes VC dimension and N is the sample size.   

```{r}



```



```{r, echo=FALSE}

```

